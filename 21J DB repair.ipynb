{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "956fb2ab",
   "metadata": {},
   "source": [
    "# Repair functions for Notebook DB setup\n",
    "\n",
    "Hosted deployment in 20J required a particular setup for Mongo and Postgres databases. This notebook undoes those."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df4d48be",
   "metadata": {},
   "source": [
    "## Postgres\n",
    "\n",
    "The updates for postgres notebooks were as follows:\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1927acd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdf1600",
   "metadata": {
    "tags": []
   },
   "source": [
    "The next group of cells set up your database connection, and reset the database to a clean state. Check notebook *08.1 Data Definition Language in SQL* if you are unsure what the next cells do.\n",
    "\n",
    "You may need to change the given values of the variables `DB_USER` and `DB_PWD`, depending on which environment you are using"
   ]
  },
  {
   "cell_type": "raw",
   "id": "801c946d",
   "metadata": {},
   "source": [
    "# If you are using the remote environment, change this cell\n",
    "# type to \"code\", and execute it\n",
    "\n",
    "DB_USER='xxx99'            # Enter your OUCU here (in quotes)\n",
    "DB_PWD='your_password'     # Enter your password here (in quotes)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9546a78",
   "metadata": {},
   "source": [
    "# If you are using a locally hosted environment, change this cell\n",
    "# type to \"code\", and execute it\n",
    "\n",
    "DB_USER='tm351'\n",
    "DB_PWD='tm351'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c997dbfe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf6593",
   "metadata": {},
   "source": [
    "## MongoDB\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4338805",
   "metadata": {},
   "source": [
    "### Setting your database credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099042e2",
   "metadata": {},
   "source": [
    "In order to work with a database, we need to create a *connection* to the database. A connection allows us to manipulate the database, and query its contents (depending on what usage rights you have been granted). For the SQL notebooks in TM351, the details of your connection will depend upon whether you are using the OU-hosted server, accessed via [tm351.open.ac.uk](https:tm351.open.ac.uk), or whether you are using a version hosted on your own computer, which you should have set up using either Vagrant or Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b18018",
   "metadata": {},
   "source": [
    "To set up the connection, you need a login name and a pasword. we will use the variables `DB_USER` and `DB_PWD` to hold the user name and password respectively that you will use to connect to the database. Run the appropriate cell to set your credentials in the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0067d0a",
   "metadata": {},
   "source": [
    "#### Connecting to the database on [tm351.open.ac.uk](https:tm351.open.ac.uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb03dbc",
   "metadata": {},
   "source": [
    "If you are using the Open University hosted server, you should execute the following cell, using your OUCU as the value of `DB_USER`, and the password you were given at the beginning of the module. Note that if the cell is in RAW NBconvert style, you will need to change its type to Code in order to execute it.\n",
    "\n",
    "The variables `DB_USER` and `DB_PWD` are strings, and so you need to put them in quotes."
   ]
  },
  {
   "cell_type": "raw",
   "id": "801717a4",
   "metadata": {},
   "source": [
    "# If you are using the remote environment, change this cell\n",
    "# type to \"code\", and execute it\n",
    "\n",
    "DB_USER='xxx99'            # Enter your OUCU here (in quotes)\n",
    "DB_PWD='your_password'     # Enter your password here (in quotes)\n",
    "\n",
    "import urllib\n",
    "\n",
    "MONGO_CONNECTION_STRING = f\"mongodb://{DB_USER}:{urllib.parse.quote_plus(DB_PWD)}@localhost:27017/?authsource=user-data\"\n",
    "print(f\"MONGO_CONNECTION_STRING = {MONGO_CONNECTION_STRING}\")\n",
    "\n",
    "DB_NAME=DB_USER\n",
    "print(f\"DB_NAME = {DB_NAME}\")\n",
    "\n",
    "ACCIDENTS_DB_NAME=\"accidents-2012\"\n",
    "# ACCIDENTS_DB_NAME=\"accidents-09-12\" # Uncomment this line to use the full accident database\n",
    "\n",
    "print(f\"ACCIDENTS_DB_NAME = {ACCIDENTS_DB_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b961c29",
   "metadata": {},
   "source": [
    "In this case, note that the connection string contains an additional option at the end: `?authsource=user-data`. For the MongoDB setup that we are using here, this option tells Mongo where to look for the authentication database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17fee45",
   "metadata": {},
   "source": [
    "#### Connecting to the database on a locally hosted machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab01c98",
   "metadata": {},
   "source": [
    "If you are running the Jupyter server on your own machine, via Docker or Vagrant, you should execute the following cell. Note that if the cell is in RAW NBconvert style, you will need to change its type to Code in order to execute it."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7933eb62",
   "metadata": {},
   "source": [
    "# If you are using a locally hosted environment, change this cell\n",
    "# type to \"code\", and execute it\n",
    "\n",
    "MONGO_CONNECTION_STRING = f\"mongodb://localhost:27017/\"\n",
    "print(f\"MONGO_CONNECTION_STRING = {MONGO_CONNECTION_STRING}\")\n",
    "\n",
    "DB_NAME=\"test_db\"  # For a local VCE, this can be any value\n",
    "print(f\"DB_NAME = {DB_NAME}\")\n",
    "\n",
    "ACCIDENTS_DB_NAME=\"accidents\"\n",
    "print(f\"ACCIDENTS_DB_NAME = {ACCIDENTS_DB_NAME}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c96d55",
   "metadata": {},
   "source": [
    "Note that the locally hosted versions of the environment give you full administrator rights, which is why you do not need to specify a user name or password. Obviously, this would not generally not be granted on a multi-user database, unless you are the database administrator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2447102d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d25420b",
   "metadata": {},
   "source": [
    "## Generic\n",
    "\n",
    "Code for managing patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aa70fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f76c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from pathlib import Path\n",
    "\n",
    "def fix_cells(cell_type, str_start, path='.',\n",
    "              replace_match=None, replace_with=None,\n",
    "              convert_to=None, overwrite=True,\n",
    "              version=nbformat.NO_CONVERT,\n",
    "              ignore_files = None,\n",
    "              verbose=False):\n",
    "    \"\"\"Remove cells of a particular type starting with a particular string.\n",
    "       Optionally replace cell contents.\n",
    "       Optionally convert cell type.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cell types\n",
    "    cell_types = ['markdown', 'code', 'raw']\n",
    "    if cell_type and cell_type not in cell_types:\n",
    "        raise ValueError('Error: cell_type not recognised')\n",
    "        \n",
    "    if convert_to and convert_to not in cell_types:\n",
    "        raise ValueError('Error: convert_to cell type not recognised')\n",
    "\n",
    "    # Iterate path\n",
    "    nb_dir = Path(path)\n",
    "    for p in nb_dir.rglob(\"*\"): #nb_dir.iterdir():\n",
    "        if ignore_files and p.name in ignore_files:\n",
    "            continue\n",
    "        if '.ipynb_checkpoints' in p.parts:\n",
    "            continue\n",
    "        \n",
    "        if p.is_file() and p.suffix == '.ipynb':\n",
    "            updated = False\n",
    "            if verbose:\n",
    "                print(f\"Checking {p}\")\n",
    "\n",
    "            # Read notebook\n",
    "            with p.open('r') as f:\n",
    "                # parse notebook\n",
    "                #nb = nbformat.read(f, as_version=nbformat.NO_CONVERT)\n",
    "                #nb = nbformat.convert(nb, version)\n",
    "                #opinionated\n",
    "                try:\n",
    "                    nb = nbformat.read(f, as_version=version)\n",
    "                except:\n",
    "                    print(f\"Failed to open: {p}\")\n",
    "                    continue\n",
    "                deletion_list = []\n",
    "                for i, cell in enumerate(nb['cells']):\n",
    "                    if cell[\"cell_type\"]==cell_type and nb['cells'][i][\"source\"].startswith(str_start):\n",
    "                        if replace_with is None and not convert_to:\n",
    "                            deletion_list.append(i)\n",
    "                        elif replace_with is not None:\n",
    "                            if replace_match:\n",
    "                                nb['cells'][i][\"source\"] = nb['cells'][i][\"source\"].replace(replace_match, replace_with)\n",
    "                                updated = True\n",
    "                            else:\n",
    "                                nb['cells'][i][\"source\"] = replace_with\n",
    "                                updated = True\n",
    "                        if convert_to:\n",
    "                            if convert_to=='code':\n",
    "                                new_cell = nbformat.v4.new_code_cell(nb['cells'][i][\"source\"])\n",
    "                                nb['cells'][i] = new_cell\n",
    "                            elif convert_to=='markdown':\n",
    "                                new_cell = nbformat.v4.new_markdown_cell(nb['cells'][i][\"source\"])\n",
    "                                nb['cells'][i] = new_cell\n",
    "                            elif convert_to=='raw':\n",
    "                                new_cell = nbformat.v4.new_raw_cell(nb['cells'][i][\"source\"])\n",
    "                                nb['cells'][i] = new_cell           \n",
    "                            else:\n",
    "                                pass\n",
    "                            updated = True\n",
    "\n",
    "                # Delete unrequired cells\n",
    "                if deletion_list:\n",
    "                    updated = True\n",
    "                nb['cells']  = [c for i, c in enumerate(nb['cells']) if i not in deletion_list]\n",
    "\n",
    "                if updated:\n",
    "                    # Validate - exception if we fail\n",
    "                    #nbformat.validate(nb)\n",
    "\n",
    "                    # Create output filename\n",
    "                    out_path =  p if overwrite else p.with_name(f'{p.stem}__patched{p.suffix}') \n",
    "\n",
    "                    # Save notebook\n",
    "                    print(f\"Updating: {p}\")\n",
    "                    nbformat.write(nb, out_path.open('w'), nbformat.NO_CONVERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f187f4",
   "metadata": {},
   "source": [
    "## Postgres cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d60f2d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking TM351_remote_VCE_test.ipynb\n",
      "Checking Using Jupyter Notebooks - READ ME FIRST.ipynb\n",
      "Checking grepping-notebooks.ipynb\n",
      "Checking Part 01 Notebooks/01.1 Getting started with IPython and Jupyter Notebooks - Bootcamp.ipynb\n",
      "Checking Part 01 Notebooks/01.2 Python recap.ipynb\n",
      "Checking Part 01 Notebooks/01.3 Basic python data structures.ipynb\n",
      "Checking Part 01 Notebooks/01.4 Defining new functions in python.ipynb\n",
      "Checking Part 01 Notebooks/01.5 Python file handling.ipynb\n",
      "Checking Part 01 Notebooks/01.X Customising the Notebook Environment.ipynb\n",
      "Checking Part 02 Notebooks/02.1 Pandas Dataframes.ipynb\n",
      "Checking Part 02 Notebooks/02.2  Data file formats.ipynb\n",
      "Checking Part 02 Notebooks/02.2.0 Data file formats - file encodings.ipynb\n",
      "Checking Part 02 Notebooks/02.2.1  Data file formats - CSV.ipynb\n",
      "Checking Part 02 Notebooks/02.2.2  Data file formats - JSON.ipynb\n",
      "Checking Part 02 Notebooks/02.2.3  Data file formats - other.ipynb\n",
      "Checking Part 03 Notebooks/03.1 Cleaning data.ipynb\n",
      "Checking Part 03 Notebooks/03.2 Selecting and projecting, sorting and limiting.ipynb\n",
      "Checking Part 03 Notebooks/03.3 Combining data from multiple datasets.ipynb\n",
      "Updating: Part 03 Notebooks/03.3 Combining data from multiple datasets.ipynb\n",
      "Checking Part 03 Notebooks/03.4 Handling missing data.ipynb\n",
      "Updating: Part 03 Notebooks/03.4 Handling missing data.ipynb\n",
      "Checking Part 04 Notebooks/04.1 Crosstabs and pivot tables.ipynb\n",
      "Checking Part 04 Notebooks/04.2 Descriptive statistics in pandas.ipynb\n",
      "Checking Part 04 Notebooks/04.3 Simple visualisations in pandas.ipynb\n",
      "Checking Part 04 Notebooks/04.4 Activity 4.4 Walkthrough.ipynb\n",
      "Checking Part 04 Notebooks/04.5 Split-apply-combine with SQL and pandas.ipynb\n",
      "Checking Part 04 Notebooks/04.5.soln SalesTeamExploration.ipynb\n",
      "Checking Part 04 Notebooks/04.6 Introducing regular expressions.ipynb\n",
      "Checking Part 04 Notebooks/04.7 Reshaping data with pandas.ipynb\n",
      "Checking Part 05 Notebooks/05.1 Anscombe's Quartet - visualising data.ipynb\n",
      "Checking Part 05 Notebooks/05.2 Getting started with maps - folium.ipynb\n",
      "Checking Part 05 Notebooks/05.3 Getting started with matplotlib.ipynb\n",
      "Checking Part 07 Notebooks/07.1 Spreadsheet basics.ipynb\n",
      "Checking Part 07 Notebooks/07.2 Problems in spreadsheet construction.ipynb\n",
      "Checking Part 08 Notebooks/08.1 Data Definition Language in SQL.ipynb\n",
      "Updating: Part 08 Notebooks/08.1 Data Definition Language in SQL.ipynb\n",
      "Checking Part 08 Notebooks/08.2 Data Manipulation Language in SQL.ipynb\n",
      "Updating: Part 08 Notebooks/08.2 Data Manipulation Language in SQL.ipynb\n",
      "Checking Part 08 Notebooks/08.3 Adding column constraints to tables.ipynb\n",
      "Updating: Part 08 Notebooks/08.3 Adding column constraints to tables.ipynb\n",
      "Checking Part 08 Notebooks/reset_databases.ipynb\n",
      "Checking Part 08 Notebooks/sql_init.ipynb\n",
      "Checking Part 09 Notebooks/09.1 Defining Foreign Keys in SQL.ipynb\n",
      "Updating: Part 09 Notebooks/09.1 Defining Foreign Keys in SQL.ipynb\n",
      "Checking Part 09 Notebooks/09.2 Using foreign keys in SQL.ipynb\n",
      "Updating: Part 09 Notebooks/09.2 Using foreign keys in SQL.ipynb\n",
      "Checking Part 09 Notebooks/09.3 Working With FOREIGN KEY Constraints.ipynb\n",
      "Updating: Part 09 Notebooks/09.3 Working With FOREIGN KEY Constraints.ipynb\n",
      "Checking Part 09 Notebooks/reset_databases.ipynb\n",
      "Checking Part 09 Notebooks/sql_init.ipynb\n",
      "Checking Part 10 Notebooks/10.1 problems with unnormalised data.ipynb\n",
      "Updating: Part 10 Notebooks/10.1 problems with unnormalised data.ipynb\n",
      "Checking Part 10 Notebooks/10.2 Normalisation - Antique opticals.ipynb\n",
      "Updating: Part 10 Notebooks/10.2 Normalisation - Antique opticals.ipynb\n",
      "Checking Part 10 Notebooks/10.3 Normalisation - the Hospital scenario.ipynb\n",
      "Updating: Part 10 Notebooks/10.3 Normalisation - the Hospital scenario.ipynb\n",
      "Checking Part 10 Notebooks/10.4 Our solution to Normalisation - the Hospital scenario.ipynb\n",
      "Updating: Part 10 Notebooks/10.4 Our solution to Normalisation - the Hospital scenario.ipynb\n",
      "Checking Part 10 Notebooks/10.5 Improvements with normalised data.ipynb\n",
      "Updating: Part 10 Notebooks/10.5 Improvements with normalised data.ipynb\n",
      "Checking Part 10 Notebooks/reset_databases.ipynb\n",
      "Checking Part 10 Notebooks/sql_init.ipynb\n",
      "Checking Part 11 Notebooks/11.0 Setting up the Movie database.ipynb\n",
      "Updating: Part 11 Notebooks/11.0 Setting up the Movie database.ipynb\n",
      "Checking Part 11 Notebooks/11.1 Movie analysis.ipynb\n",
      "Updating: Part 11 Notebooks/11.1 Movie analysis.ipynb\n",
      "Checking Part 11 Notebooks/11.2 subqueries as value and set.ipynb\n",
      "Updating: Part 11 Notebooks/11.2 subqueries as value and set.ipynb\n",
      "Checking Part 11 Notebooks/11.3 Subqueries as tables.ipynb\n",
      "Updating: Part 11 Notebooks/11.3 Subqueries as tables.ipynb\n",
      "Checking Part 11 Notebooks/11.4 Views.ipynb\n",
      "Updating: Part 11 Notebooks/11.4 Views.ipynb\n",
      "Checking Part 11 Notebooks/11.5 Six degrees of Bacon.ipynb\n",
      "Updating: Part 11 Notebooks/11.5 Six degrees of Bacon.ipynb\n",
      "Checking Part 11 Notebooks/SQL_cheatsheet.ipynb\n",
      "Checking Part 11 Notebooks/reset_databases.ipynb\n",
      "Checking Part 11 Notebooks/sql_init.ipynb\n",
      "Checking Part 12 Notebooks/12.1 Concurrent Transactions.ipynb\n",
      "Updating: Part 12 Notebooks/12.1 Concurrent Transactions.ipynb\n",
      "Checking Part 12 Notebooks/12.2 Transaction anomalies.ipynb\n",
      "Updating: Part 12 Notebooks/12.2 Transaction anomalies.ipynb\n",
      "Checking Part 12 Notebooks/reset_databases.ipynb\n",
      "Checking Part 12 Notebooks/sql_init.ipynb\n",
      "Checking Part 12 Notebooks/optional_part_12/12.3 Optional- Concurrent Transactions and Multiple Threads.ipynb\n",
      "Checking Part 12 Notebooks/optional_part_12/12.3a Gibson.ipynb\n",
      "Updating: Part 12 Notebooks/optional_part_12/12.3a Gibson.ipynb\n",
      "Checking Part 12 Notebooks/optional_part_12/12.3a Paxton.ipynb\n",
      "Updating: Part 12 Notebooks/optional_part_12/12.3a Paxton.ipynb\n",
      "Checking Part 12 Notebooks/optional_part_12/reset_databases.ipynb\n",
      "Checking Part 12 Notebooks/optional_part_12/sql_init.ipynb\n",
      "Checking Part 14 Notebooks/14.1 Basic CRUD.ipynb\n",
      "Updating: Part 14 Notebooks/14.1 Basic CRUD.ipynb\n",
      "Checking Part 14 Notebooks/14.2 Working With Embedded Documents.ipynb\n",
      "Updating: Part 14 Notebooks/14.2 Working With Embedded Documents.ipynb\n",
      "Checking Part 14 Notebooks/14.3 Importing Data into MongoDB.ipynb\n",
      "Updating: Part 14 Notebooks/14.3 Importing Data into MongoDB.ipynb\n",
      "Checking Part 14 Notebooks/14.4 Introduction to the accidents database.ipynb\n",
      "Updating: Part 14 Notebooks/14.4 Introduction to the accidents database.ipynb\n",
      "Checking Part 14 Notebooks/14.5 Investigating the accident data.ipynb\n",
      "Updating: Part 14 Notebooks/14.5 Investigating the accident data.ipynb\n",
      "Checking Part 14 Notebooks/14.A Optional- Using statistical tests - correlation.ipynb\n",
      "Updating: Part 14 Notebooks/14.A Optional- Using statistical tests - correlation.ipynb\n",
      "Checking Part 14 Notebooks/14.B Optional- Using statistical tests - regression.ipynb\n",
      "Updating: Part 14 Notebooks/14.B Optional- Using statistical tests - regression.ipynb\n",
      "Checking Part 15 Notebooks/15.1 Mapping accidents.ipynb\n",
      "Updating: Part 15 Notebooks/15.1 Mapping accidents.ipynb\n",
      "Checking Part 15 Notebooks/15.2 Searching within a geographical area.ipynb\n",
      "Updating: Part 15 Notebooks/15.2 Searching within a geographical area.ipynb\n",
      "Checking Part 15 Notebooks/15.3 Introducing aggregation pipelines.ipynb\n",
      "Updating: Part 15 Notebooks/15.3 Introducing aggregation pipelines.ipynb\n",
      "Checking Part 15 Notebooks/15.4 Grouping and summarising operations in aggregation pipelines.ipynb\n",
      "Updating: Part 15 Notebooks/15.4 Grouping and summarising operations in aggregation pipelines.ipynb\n",
      "Checking Part 15 Notebooks/15.5 Introducing the Roads collection.ipynb\n",
      "Updating: Part 15 Notebooks/15.5 Introducing the Roads collection.ipynb\n",
      "Checking Part 15 Notebooks/15.6 Working with roads location data.ipynb\n",
      "Updating: Part 15 Notebooks/15.6 Working with roads location data.ipynb\n",
      "Checking Part 16 Notebooks/16.1 Accidents over time.ipynb\n",
      "Updating: Part 16 Notebooks/16.1 Accidents over time.ipynb\n",
      "Checking Part 16 Notebooks/16.2 Python map-reduce.ipynb\n",
      "Checking Part 16 Notebooks/16.3 Introducing map-reduce in MongoDB Database Queries.ipynb\n",
      "Updating: Part 16 Notebooks/16.3 Introducing map-reduce in MongoDB Database Queries.ipynb\n",
      "Checking Part 20 Notebooks/20.1 The k-nearest neighbours classifier.ipynb\n",
      "Checking Part 20 Notebooks/20.2 The leave-one-out algorithm.ipynb\n",
      "Checking Part 21 Notebooks/21.1 The k-means algorithm.ipynb\n",
      "Checking Part 21 Notebooks/21.2 k-means clustering - choosing initial values.ipynb\n",
      "Checking Part 21 Notebooks/21.3 Visualising clusters with silhouette coefficients.ipynb\n",
      "Checking Part 22 Notebooks/22.1 Case study preliminaries - the vector space model.ipynb\n",
      "Checking Part 22 Notebooks/22.2 Preliminaries - building the classifier.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Part 22 Notebooks/22.3 Applying the classifier to a real dataset.ipynb\n",
      "Checking Part 22 Notebooks/22.4 Term frequency and inverse document frequency.ipynb\n",
      "Checking Part 23 Notebooks/23.1  SQL injection hacks.ipynb\n",
      "Checking Part 23 Notebooks/form_server.ipynb\n",
      "Checking Part 23 Notebooks/form_server_safe.ipynb\n",
      "Checking Part 23 Notebooks/part_23_authentication_notebook.ipynb\n",
      "Checking Part 23 Notebooks/reset_form_server.ipynb\n",
      "Checking Part 25 Notebooks/25.1 Exploring the RDFLib package.ipynb\n",
      "Checking Part 25 Notebooks/25.2 Querying using SPARQL.ipynb\n",
      "Checking Part 25 Notebooks/25.3 Endpoints - accessing real data.ipynb\n",
      "Checking Part 26 Notebooks/26.1 Using multiple endpoints.ipynb\n",
      "Checking Part 26 Notebooks/26.2 The SPARQL CONSTRUCT query and inferencing.ipynb\n",
      "Checking Part 26 Notebooks/26.3 Visualisation.ipynb\n"
     ]
    }
   ],
   "source": [
    "ignore_files=['21J DB repair.ipynb']\n",
    "\n",
    "str_start = '# If you are using the remote environment, change this cell'\n",
    "fix_cells('raw', str_start, ignore_files=ignore_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74808740",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_start = \"# If you are using a locally hosted environment, change this cell\"\n",
    "replace_match =\"\"\"# If you are using a locally hosted environment, change this cell\n",
    "# type to \"code\", and execute it\n",
    "\n",
    "\"\"\"\n",
    "replace_with = ''\n",
    "convert_to = 'code'\n",
    "fix_cells('raw', str_start, convert_to='code',\n",
    "          replace_match=replace_match, replace_with=replace_with, ignore_files=ignore_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c1ee95",
   "metadata": {},
   "source": [
    "*Minor other changes noticed in notebooks 3.3 and 3.4 preview of git diffs and addressed manually.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
